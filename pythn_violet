import os
import re
import requests
import pandas as pd
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from duckduckgo_search import DDGS
from googlesearch import search
from tkinter import filedialog
import pytesseract
from PIL import Image
import zipfile
import rarfile
import fitz  # PyMuPDF
import textract
import tempfile

# Tá»« khoÃ¡ tÃ¬m kiáº¿m chÃ­nh
KEYWORDS = [
    "danh sÃ¡ch há»c sinh", "tuyá»ƒn sinh lá»›p 1", "tra cá»©u há»c sinh",
    "káº¿t quáº£ tuyá»ƒn sinh", "ná»™p há»“ sÆ¡ lá»›p 6"
]

# Biá»ƒu thá»©c nháº­n dáº¡ng SÄT
PHONE_PATTERN = re.compile(r'(0|\+84)[\s\.\-]?\d{2,3}[\s\.\-]?\d{3}[\s\.\-]?\d{3,4}')

# Táº¡o thÆ° má»¥c táº¡m lÆ°u káº¿t quáº£
output_folder = filedialog.askdirectory(title="Chá»n thÆ° má»¥c lÆ°u káº¿t quáº£")
os.makedirs(output_folder, exist_ok=True)

# HÃ m tÃ¬m kiáº¿m URL theo Ä‘á»‹a chá»‰
def search_links(location_keyword):
    combined_keywords = [f"{kw} {location_keyword}" for kw in KEYWORDS]
    links = set()
    for kw in combined_keywords:
        print(f"ğŸ” TÃ¬m kiáº¿m: {kw}")
        # Google
        try:
            for url in search(kw, num_results=5):
                links.add(url)
        except:
            pass
        # DuckDuckGo
        try:
            with DDGS() as ddgs:
                results = ddgs.text(kw, max_results=5)
                for r in results:
                    links.add(r.get("href", ""))
        except:
            pass
    return list(links)

# Táº£i file vá»
def download_file(url, save_dir):
    local_filename = os.path.join(save_dir, url.split("/")[-1])
    try:
        with requests.get(url, stream=True, timeout=10) as r:
            r.raise_for_status()
            with open(local_filename, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    f.write(chunk)
        return local_filename
    except:
        return None

# TrÃ­ch xuáº¥t ná»™i dung tá»« file
def extract_text(filepath):
    try:
        ext = os.path.splitext(filepath)[1].lower()
        if ext in ['.pdf']:
            text = ""
            doc = fitz.open(filepath)
            for page in doc:
                text += page.get_text()
            return text
        elif ext in ['.docx', '.txt']:
            return textract.process(filepath).decode("utf-8")
        elif ext in ['.xlsx', '.xls', '.csv']:
            df = pd.read_excel(filepath) if ext != '.csv' else pd.read_csv(filepath)
            return "\n".join(df.astype(str).fillna("").values.ravel())
        elif ext in ['.jpg', '.png']:
            return pytesseract.image_to_string(Image.open(filepath))
        elif ext in ['.zip']:
            return extract_from_zip(filepath)
        elif ext in ['.rar']:
            return extract_from_rar(filepath)
    except:
        return ""

# Giáº£i nÃ©n zip
def extract_from_zip(zip_path):
    temp_dir = tempfile.mkdtemp()
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(temp_dir)
    return "\n".join([extract_text(os.path.join(temp_dir, f)) for f in os.listdir(temp_dir)])

# Giáº£i nÃ©n rar
def extract_from_rar(rar_path):
    temp_dir = tempfile.mkdtemp()
    with rarfile.RarFile(rar_path) as rf:
        rf.extractall(temp_dir)
    return "\n".join([extract_text(os.path.join(temp_dir, f)) for f in os.listdir(temp_dir)])

# TrÃ­ch xuáº¥t dÃ²ng cÃ³ chá»©a sá»‘ Ä‘iá»‡n thoáº¡i
def extract_valid_lines(text):
    lines = []
    for line in text.split('\n'):
        if PHONE_PATTERN.search(line):
            lines.append(line.strip())
    return lines

# Xá»­ lÃ½ tá»«ng URL
def process_link(url, output_excel_path):
    print(f"âš™ï¸ Äang xá»­ lÃ½: {url}")
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, 'html.parser')
        attachments = soup.find_all('a', href=True)
        lines = []

        for a in attachments:
            href = a['href']
            full_url = urljoin(url, href)
            if any(href.lower().endswith(ext) for ext in ['.pdf', '.docx', '.xlsx', '.xls', '.csv', '.txt', '.jpg', '.png', '.zip', '.rar']):
                filepath = download_file(full_url, output_folder)
                if filepath:
                    text = extract_text(filepath)
                    lines.extend(extract_valid_lines(text))

        # Náº¿u khÃ´ng cÃ³ tá»‡p => Ä‘á»c ná»™i dung HTML
        if not lines:
            html_text = soup.get_text()
            lines = extract_valid_lines(html_text)

        # Táº¡o báº£ng dá»¯ liá»‡u
        data = []
        for i, line in enumerate(lines, 1):
            phone = PHONE_PATTERN.search(line)
            data.append([i, "Äang xá»­ lÃ½", "", "", phone.group() if phone else ""])

        # Xuáº¥t ra Excel
        df = pd.DataFrame(data, columns=["STT", "Há»Œ TÃŠN", "NÄ‚M SINH", "Há»Œ TÃŠN CHA/Máº¸", "Sá» ÄIá»†N THOáº I CHA/Máº¸"])
        df.to_excel(output_excel_path, index=False)
        print(f"âœ… ÄÃ£ lÆ°u: {output_excel_path}")
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")

# MAIN: há»i tá»« khoÃ¡ ngÆ°á»i dÃ¹ng vÃ  báº¯t Ä‘áº§u xá»­ lÃ½
if __name__ == "__main__":
    location = input("Nháº­p tá»« khoÃ¡ Ä‘á»‹a chá»‰ (vÃ­ dá»¥: TP BÃ  Rá»‹a): ").strip()
    links = search_links(location)
    print(f"ğŸ”— TÃ¬m tháº¥y {len(links)} liÃªn káº¿t")

    for idx, link in enumerate(links):
        output_path = os.path.join(output_folder, f"OUTPUT_EXCEL_{idx+1}.xlsx")
        process_link(link, output_path)

    print("ğŸ¯ HoÃ n táº¥t thu tháº­p.")